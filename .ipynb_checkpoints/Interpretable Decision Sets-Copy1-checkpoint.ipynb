{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from apyori import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# itemset (each rule predicate --> class label)\n",
    "class rule:\n",
    "    \n",
    "    def __init__(self,feature_list,value_list,class_label):\n",
    "        self.itemset = set()\n",
    "        self.class_label = None\n",
    "        self.add_item(feature_list,value_list)\n",
    "        self.set_class_label(class_label)\n",
    "    \n",
    "    def add_item(self,feature_list,value_list):\n",
    "        \n",
    "        if len(feature_list) != len(value_list):\n",
    "            print(\"Some error in inputting feature value pairs\")\n",
    "            return\n",
    "        for i in range(0,len(feature_list)):\n",
    "            self.itemset.add((feature_list[i],value_list[i]))\n",
    "    \n",
    "    def print_rule(self):\n",
    "        s = \"If \"\n",
    "        for item in self.itemset:\n",
    "            s += str(item[0]) + \" == \" +str(item[1]) + \" and \"\n",
    "        s = s[:-5]\n",
    "        s += \", then \"\n",
    "        s += str(self.class_label)\n",
    "        print(s)\n",
    "        \n",
    "    def all_predicates_same(self, r):\n",
    "        return self.itemset == r.itemset\n",
    "    \n",
    "    def class_label_same(self,r):\n",
    "        return self.class_label == r.class_label\n",
    "            \n",
    "    def set_class_label(self,label):\n",
    "        self.class_label = label\n",
    "        \n",
    "    def get_length(self):\n",
    "        return len(self.itemset)\n",
    "    \n",
    "    def get_cover(self, df):\n",
    "        dfnew = df.copy()\n",
    "        for pattern in self.itemset: \n",
    "            dfnew = dfnew[dfnew[pattern[0]] == pattern[1]]\n",
    "        return list(dfnew.index.values)\n",
    "\n",
    "    def get_correct_cover(self, df, Y):\n",
    "        indexes_points_covered = self.get_cover(df) # indices of all points satisfying the rule\n",
    "        Y_arr = pd.Series(Y)                    # make a series of all Y labels\n",
    "        labels_covered_points = list(Y_arr[indexes_points_covered])   # get a list only of Y labels of the points covered\n",
    "        correct_cover = []\n",
    "        for ind in range(0,len(labels_covered_points)):\n",
    "            if labels_covered_points[ind] == self.class_label:\n",
    "                correct_cover.append(indexes_points_covered[ind])\n",
    "        return correct_cover, indexes_points_covered\n",
    "    \n",
    "    def get_incorrect_cover(self, df, Y):\n",
    "        correct_cover, full_cover = self.get_correct_cover(df, Y)\n",
    "        return (sorted(list(set(full_cover) - set(correct_cover))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_apriori(df, support_thres):\n",
    "    # the idea is to basically make a list of strings out of df and run apriori api on it \n",
    "    # return the frequent itemsets\n",
    "    dataset = []\n",
    "    for i in range(0,df.shape[0]):\n",
    "        temp = []\n",
    "        for col_name in df.columns:\n",
    "            temp.append(col_name+\"=\"+str(df[col_name][i]))\n",
    "        dataset.append(temp)\n",
    "\n",
    "    results = list(apriori(dataset, min_support=support_thres))\n",
    "    \n",
    "    list_itemsets = []\n",
    "    for ele in results:\n",
    "        temp = []\n",
    "        for pred in ele.items:\n",
    "            temp.append(pred)\n",
    "        list_itemsets.append(temp)\n",
    "\n",
    "    return list_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createrules(freq_itemsets, labels_set):\n",
    "    # create a list of rule objects from frequent itemsets \n",
    "    list_of_rules = []\n",
    "    for one_itemset in freq_itemsets:\n",
    "        feature_list = []\n",
    "        value_list = []\n",
    "        for pattern in one_itemset:\n",
    "            fea_val = pattern.split(\"=\")\n",
    "            feature_list.append(fea_val[0])\n",
    "            value_list.append(fea_val[1])\n",
    "        for each_label in labels_set:\n",
    "            temp_rule = rule(feature_list,value_list,each_label)\n",
    "            list_of_rules.append(temp_rule)\n",
    "\n",
    "    return list_of_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_rule_length(list_rules):\n",
    "    len_arr = []\n",
    "    for r in list_rules:\n",
    "        len_arr.append(r.get_length())\n",
    "    return max(len_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overlap(r1, r2, df):\n",
    "    return sorted(list(set(r1.get_cover(df)).intersection(set(r2.get_cover(df)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def func_evaluation(soln_set, list_rules, df, Y, lambda_array):\n",
    "    # evaluate the objective function based on rules in solution set \n",
    "    # soln set is a set of indexes which when used to index elements in list_rules point to the exact rules in the solution set\n",
    "    # compute f1 through f7 and we assume there are 7 lambdas in lambda_array\n",
    "    f = [] #stores values of f1 through f7; \n",
    "    \n",
    "    # f0 term\n",
    "    f0 = len(list_rules) - len(soln_set) # |S| - size(R)\n",
    "    f.append(f0)\n",
    "    \n",
    "    # f1 term\n",
    "    Lmax = max_rule_length(list_rules)\n",
    "    sum_rule_length = 0.0\n",
    "    for rule_index in soln_set:\n",
    "        sum_rule_length += list_rules[rule_index].get_length()\n",
    "    \n",
    "    f1 = Lmax * len(list_rules) - sum_rule_length\n",
    "    f.append(f1)\n",
    "    \n",
    "    # f2 term - intraclass overlap\n",
    "    sum_overlap_intraclass = 0.0\n",
    "    for r1_index in soln_set:\n",
    "        for r2_index in soln_set:\n",
    "            if r1_index >= r2_index:\n",
    "                continue\n",
    "            if list_rules[r1_index].class_label == list_rules[r2_index].class_label:\n",
    "                sum_overlap_intraclass += len(overlap(list_rules[r1_index], list_rules[r2_index],df))\n",
    "    f2 = df.shape[0] * len(list_rules) * len(list_rules) - sum_overlap_intraclass\n",
    "    f.append(f2)\n",
    "    \n",
    "    # f3 term - interclass overlap\n",
    "    sum_overlap_interclass = 0.0\n",
    "    for r1_index in soln_set:\n",
    "        for r2_index in soln_set:\n",
    "            if r1_index >= r2_index:\n",
    "                continue\n",
    "            if list_rules[r1_index].class_label != list_rules[r2_index].class_label:\n",
    "                sum_overlap_interclass += len(overlap(list_rules[r1_index], list_rules[r2_index],df))\n",
    "    f3 = df.shape[0] * len(list_rules) * len(list_rules) - sum_overlap_interclass\n",
    "    f.append(f3)\n",
    "    \n",
    "    # f4 term - coverage of all classes\n",
    "    classes_covered = set() # set\n",
    "    for index in soln_set:\n",
    "        classes_covered.add(list_rules[index].class_label)\n",
    "    f4 = len(classes_covered)\n",
    "    f.append(f4)\n",
    "    \n",
    "    # f5 term - accuracy\n",
    "    sum_incorrect_cover = 0.0\n",
    "    for index in soln_set:\n",
    "        sum_incorrect_cover += len(list_rules[index].get_incorrect_cover(df,Y))\n",
    "    f5 = df.shape[0] * len(list_rules) - sum_incorrect_cover\n",
    "    f.append(f5)\n",
    "    \n",
    "    #f6 term - cover correctly with at least one rule\n",
    "    atleast_once_correctly_covered = set()\n",
    "    for index in soln_set:\n",
    "        correct_cover, full_cover = list_rules[index].get_correct_cover(df,Y)\n",
    "        atleast_once_correctly_covered = atleast_once_correctly_covered.union(set(correct_cover))\n",
    "    f6 = len(atleast_once_correctly_covered)\n",
    "    f.append(f6)\n",
    "    \n",
    "    obj_val = 0.0\n",
    "    for i in range(7):\n",
    "        obj_val += f[i] * lambda_array[i]\n",
    "    \n",
    "    #print(f)\n",
    "    return obj_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_random_set(soln_set, delta, len_list_rules):\n",
    "    all_rule_indexes = set(range(len_list_rules))\n",
    "    return_set = set()\n",
    "    \n",
    "    # sample in-set elements with prob. (delta + 1)/2\n",
    "    p = (delta + 1.0)/2\n",
    "    for item in soln_set:\n",
    "        random_val = np.random.uniform()\n",
    "        if random_val <= p:\n",
    "            return_set.add(item)\n",
    "    \n",
    "    # sample out-set elements with prob. (1 - delta)/2\n",
    "    p_prime = (1.0 - delta)/2\n",
    "    for item in (all_rule_indexes - soln_set):\n",
    "        random_val = np.random.uniform()\n",
    "        if random_val <= p_prime:\n",
    "            return_set.add(item)\n",
    "    \n",
    "    #print(soln_set)\n",
    "    #print(all_rule_indexes - soln_set)\n",
    "    return return_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_omega_for_element(soln_set, delta, rule_x_index, list_rules, df, Y, lambda_array, error_threshold):\n",
    "    #assumes rule_x_index is not in soln_set \n",
    "    \n",
    "    Exp1_func_vals = []\n",
    "    \n",
    "    Exp2_func_vals = []\n",
    "    \n",
    "    while(True):\n",
    "        \n",
    "        # first expectation term (include x)\n",
    "        for i in range(10):\n",
    "            temp_soln_set = sample_random_set(soln_set, delta, len(list_rules))\n",
    "            temp_soln_set.add(rule_x_index)\n",
    "            Exp1_func_vals.append(func_evaluation(temp_soln_set, list_rules, df, Y, lambda_array))\n",
    "        \n",
    "        # second expectation term (exclude x)\n",
    "        for j in range(10):\n",
    "            temp_soln_set = sample_random_set(soln_set, delta, len(list_rules))\n",
    "            if rule_x_index in temp_soln_set:\n",
    "                temp_soln_set.remove(rule_x_index)\n",
    "            Exp2_func_vals.append(func_evaluation(temp_soln_set, list_rules, df, Y, lambda_array))\n",
    "    \n",
    "        # compute standard error of mean difference\n",
    "        variance_Exp1 = np.var(Exp1_func_vals, dtype=np.float64)\n",
    "        variance_Exp2 = np.var(Exp2_func_vals, dtype=np.float64)\n",
    "        std_err = math.sqrt(variance_Exp1/len(Exp1_func_vals) + variance_Exp2/len(Exp2_func_vals))\n",
    "        print(\"Standard Error \"+str(std_err))\n",
    "        \n",
    "        if std_err <= error_threshold:\n",
    "            break\n",
    "            \n",
    "    return np.mean(Exp1_func_vals) - np.mean(Exp2_func_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_OPT(list_rules, df, Y, lambda_array):\n",
    "    opt_set = set()\n",
    "    for i in range(len(list_rules)):\n",
    "        r_val = np.random.uniform()\n",
    "        if r_val <= 0.5:\n",
    "            opt_set.add(i)\n",
    "    return func_evaluation(opt_set, list_rules, df, Y, lambda_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def smooth_local_search(list_rules, df, Y, lambda_array, delta, delta_prime):\n",
    "    # step by step implementation of smooth local search algorithm in the \n",
    "    # FOCS paper: https://people.csail.mit.edu/mirrokni/focs07.pdf (page 6)\n",
    "    \n",
    "    # step 1: set the value n and OPT; initialize soln_set to empty\n",
    "    n = len(list_rules)\n",
    "    OPT = compute_OPT(list_rules, df, Y, lambda_array)\n",
    "    print(\"2/n*n OPT value is \"+str(2.0/(n*n)*OPT))\n",
    "    \n",
    "    soln_set = set()\n",
    "    \n",
    "    restart_omega_computations = False\n",
    "    \n",
    "    while(True):\n",
    "    \n",
    "        # step 2 & 3: for each element estimate omega within certain error_threshold; if estimated omega > 2/n^2 * OPT, then add \n",
    "        # the corresponding rule to soln set and recompute omega estimates again\n",
    "        omega_estimates = []\n",
    "        for rule_x_index in range(n):\n",
    "                \n",
    "            print(\"Estimating omega for rule \"+str(rule_x_index))\n",
    "            omega_est = estimate_omega_for_element(soln_set, delta, rule_x_index, list_rules, df, Y, lambda_array, 1.0/(n*n) * OPT)\n",
    "            omega_estimates.append(omega_est)\n",
    "            print(\"Omega estimate is \"+str(omega_est))\n",
    "            \n",
    "             if rule_x_index in soln_set:\n",
    "                continue\n",
    "            \n",
    "            if omega_est > 0.5/(n*n) * OPT:\n",
    "                # add this element to solution set and recompute omegas\n",
    "                soln_set.add(rule_x_index)\n",
    "                restart_omega_computations = True\n",
    "                print(\"-----------------------\")\n",
    "                print(\"Adding to the solution set rule \"+str(rule_x_index))\n",
    "                print(\"-----------------------\")\n",
    "                break    \n",
    "        \n",
    "        if restart_omega_computations: \n",
    "            restart_omega_computations = False\n",
    "            continue\n",
    "            \n",
    "        # reaching this point of code means there is nothing more to add to the solution set, but we can remove elements\n",
    "        for rule_ind in soln_set:\n",
    "            if omega_estimates[rule_ind] < -0.5/(n*n) * OPT:\n",
    "                soln_set.remove(rule_ind)\n",
    "                restart_omega_computations = True\n",
    "                \n",
    "                print(\"Removing from the solution set rule \"+str(rule_ind))\n",
    "                break\n",
    "                \n",
    "        if restart_omega_computations: \n",
    "            restart_omega_computations = False\n",
    "            continue\n",
    "            \n",
    "        # reaching here means there is no element to add or remove from the solution set\n",
    "        return sample_random_set(soln_set, delta_prime, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic_train.tab',' ', header=None, names=['Passenger_Cat', 'Age_Cat', 'Gender'])\n",
    "df1 = pd.read_csv('titanic_train.Y', ' ', header=None, names=['Died', 'Survived'])\n",
    "Y = list(df1['Died'].values)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "itemsets = run_apriori(df, 0.2)\n",
    "list_of_rules = createrules(itemsets, list(set(Y)))\n",
    "print(\"----------------------\")\n",
    "for r in list_of_rules:\n",
    "    r.print_rule()\n",
    "\n",
    "lambda_array = [1.0]*7     # use separate hyperparamter search routine\n",
    "s1 = smooth_local_search(list_of_rules, df, Y, lambda_array, 0.33, 0.33)\n",
    "s2 = smooth_local_search(list_of_rules, df, Y, lambda_array, 0.33, -1.0)\n",
    "f1 = func_evaluation(s1, list_of_rules, df, Y, lambda_array)\n",
    "f2 = func_evaluation(s2, list_of_rules, df, Y, lambda_array)\n",
    "if f1 > f2:\n",
    "    print(\"The Solution Set is: \"+str(s1))\n",
    "else:\n",
    "    print(\"The Solution Set is: \"+str(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# testing rule class\n",
    "it = rule(['Coach_Class', 'Gender'], ['crew', 'male'], 'Died')\n",
    "it1 = rule(['Gender', 'Coach_Class'], ['male','crew'], 'Died')\n",
    "#it.add_item(['Coach_Class', 'Age_Cat'], ['crew', 'adult'])\n",
    "it.print_rule()\n",
    "it1.print_rule()\n",
    "it.all_predicates_same(it1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_random_set({1,3}, 0.33, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = list_of_rules[33]\n",
    "r.print_rule()\n",
    "\n",
    "dfnew = df.copy()\n",
    "for pattern in r.itemset: \n",
    "    print(pattern)\n",
    "    dfnew = dfnew[dfnew[pattern[0]] == pattern[1]]\n",
    "    print(dfnew.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "\n",
    "transactions = [\n",
    "    ['beer', 'nuts'],\n",
    "    ['beer', 'cheese'],\n",
    "    ['beer', 'cheese']\n",
    "]\n",
    "results = list(apriori(transactions))\n",
    "str(results[0].items).replace(\"frozenset(\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pd.Series([-2, 1, 5, 3, 8, 5, 6])\n",
    "b = [1, 2, 5]\n",
    "c = a[b]\n",
    "list(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = {1, 2, 3}\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = [3, 2, 1]\n",
    "sorted(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1 = [1, 2, 3]\n",
    "l2 = [1, 2]\n",
    "s = set(l1).intersection(set(l2))\n",
    "s.union({4,5})\n",
    "s.add(1)\n",
    "s.remove(2)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.uniform()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
